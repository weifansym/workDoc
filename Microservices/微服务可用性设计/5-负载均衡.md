## 负载均衡
数据中心内部的负载均衡

在理想情况下，某个服务的负载会完全均匀地分发给所有的后端任务。在任何时刻，最忙和最不忙的节点永远消耗同样数量的CPU。
![image](https://user-images.githubusercontent.com/6757408/194712553-78942b02-fafe-4b17-9391-ec57b07fc81c.png)

目标：
* 均衡的流量分发。
* 可靠的识别异常节点。
* scale-out，增加同质节点扩容。
* 减少错误，提高可用性。

我们发现在 backend 之间的 load 差异比较大：
![image](https://user-images.githubusercontent.com/6757408/194712579-b13b9597-0a35-4970-af65-19cd0665debe.png)

我们希望是右图的效果，每个节点间的cpu负载比较稳定，实际却差异非常大，如左图，原因如下：
* 每个请求的处理成本不同。
* 物理机环境的差异:
  * 服务器很难强同质性。
  * 存在共享资源争用（内存缓存、带宽、IO等）。
* 性能因素:
  * FullGC。
  * JVM JIT。

参考JSQ（最闲轮训）负载均衡算法带来的问题，缺乏的是服务端全局视图，因此我们目标需要综合考虑：负载*+*可用性。
![image](https://user-images.githubusercontent.com/6757408/194712657-0ee25f82-50a1-40d2-9153-3e6a7675e9a7.png)

如果参考JSQ，LBA下次请求会选Server Y，而从全局看，应该打到Server X，缺乏全局视图
![image](https://user-images.githubusercontent.com/6757408/194712674-e4bd848c-84d5-479f-8efa-75fa3a473a59.png)

如上图：节点间cpu差距就会很大

参考了[《The power of two choices in randomized load balancing》](https://ieeexplore.ieee.org/document/963420)的思路，我们使用 p2c 算法，随机选取的两个节点进行打分，选择更优的节点:
* 选择 backend：CPU，client：health（健康度）、inflight（当前正在有多少个请求）、latency（延迟） 作为指标，使用一个简单的线性方程进行打分。
* 对新启动的节点使用常量惩罚值（penalty），以及使用探针方式最小化放量，进行预热**。
* 打分比较低的节点，避免进入“永久黑名单”而无法恢复，使用统计衰减的方式，让节点指标逐渐恢复到初始状态*(即默认值)*。
* 当前发出去的请求超过了 predict lagtency，就会加惩罚。

> 指标计算结合 moving average，使用时间衰减，计算vt = v(t-1) * β + at * (1-β) ，β 为若干次幂的倒数即: Math.Exp((-span) / 600ms)

https://github.com/go-kratos/kratos/tree/v1.0.x/pkg/net/rpc/warden/balancer/p2c

### 最佳实践
* 变更管理:
  * 70％的问题是由变更引起的，恢复可用代码并不总是坏事（快速回滚）。
* 避免过载:
  * 过载保护、流量调度等。
* 依赖管理:
  * 任何依赖都可能故障，做 chaos monkey testing**，注入故障测试。
* 优雅降级:
  * 有损服务，避免核心链路依赖故障。
* 重试退避:
  * 退让算法，冻结时间，**API retry detail 控制策略**。
* 超时控制:
  * 进程内 + 服务间 超时控制。
* 极限压测 + 故障演练。
* 扩容 + 重启 + 消除有害流量。

### References
* http://www.360doc.com/content/16/1124/21/31263000_609259745.shtml
* http://www.infoq.com/cn/articles/basis-frameworkto-implement-micro-service/
* http://www.infoq.com/cn/news/2017/04/linkerd-celebrates-one-year
* https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c
* https://mp.weixin.qq.com/s?__biz=MzAwNjQwNzU2NQ==&mid=402841629&idx=1&sn=f598fec9b370b8a6f2062233b31122e0&mpshare=1&scene=23&srcid=0404qP0fH8zRiIiFzQBiuzuU#rd
* https://mp.weixin.qq.com/s?__biz=MzIzMzk2NDQyMw==&mid=2247486641&idx=1&sn=1660fb41b0c5b8d8d6eacdfc1b26b6a6&source=41#wechat_redirect
* https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/
* https://www.cs.columbia.edu/~ruigu/papers/socc18-final100.pdf
* https://github.com/alibaba/Sentinel/wiki/系统负载保护
* https://blog.csdn.net/okiwilldoit/article/details/81738782
* http://alex-ii.github.io/notes/2019/02/13/predictive_load_balancing.html
* https://blog.csdn.net/m0_38106113/article/details/81542863



